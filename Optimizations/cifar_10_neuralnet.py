# -*- coding: utf-8 -*-
"""CIFAR-10: NeuralNet

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c6-YZHbeiTqWyDNPxMPYHyR9ZTR7HL7o
"""

import torch
from torchvision import datasets, transforms
from torchvision.datasets import CIFAR10
from torch.utils.data import DataLoader
import torch.nn as nn
import torch.nn.functional as F

print(torch.cuda.is_available())

"""## Importing the Dataset with Normalization

1. `transform`: creates a pipeline of pre-processsing steps that is to be applied to the MNIST dataset. It converts the PIL image to pytorch tensor and scales pixel values.

$$  normalized= \frac{tensor-mean}{std}$$



"""

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize RGB channels
])

train_set = CIFAR10(root='./data', train=True, transform=transform, download=True)
test_set = CIFAR10(root='./data', train=False, transform=transform, download=True)

train_loader = DataLoader(train_set, batch_size=128, shuffle=True)
test_loader = DataLoader(test_set, batch_size=1000)

"""## MNIST NET"""

class CIFARNet(nn.Module):
    def __init__(self): #initializing the nn
        super().__init__()
        self.fc1 = nn.Linear(3*32*32, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, 128)
        self.fc4 = nn.Linear(128, 10)

    def forward(self, x):
        x = x.view(-1, 3*32*32)  # flatten
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        return self.fc4(x)  # logits

class CustomOptimizer: # REPLACED WITH OTHER OPTIMIZERS FOR TESTING-
    def __init__(self, params, lr=0.01):
        self.params = list(params)
        self.lr = lr

    def step(self):
        for p in self.params:
            if p.grad is not None:
                p.data -= self.lr * p.grad

    def zero_grad(self):
        for p in self.params:
            if p.grad is not None:
                p.grad.zero_()

"""## TRAINING"""

def train(model, optimizer, loss_fn, train_loader, device):
    model.train()
    for batch_idx, (X, y) in enumerate(train_loader):
        X, y = X.to(device), y.to(device)

        logits = model(X)
        loss = loss_fn(logits, y)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if batch_idx % 100 == 0:
            print(f"Batch {batch_idx}: Loss = {loss.item():.4f}")

"""## EVALUATION"""

def test(model, test_loader, device):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for X, y in test_loader:
            X, y = X.to(device), y.to(device)
            logits = model(X)
            pred = logits.argmax(dim=1)
            correct += (pred == y).sum().item()
            total += y.size(0)
    print(f"Test Accuracy: {correct / total:.4f}")

if __name__ == "__main__":
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    model = CIFARNet().to(device)
    optimizer = CustomOptimizer(model.parameters(), lr=0.01)
    loss_fn = nn.CrossEntropyLoss()

    for epoch in range(1, 10):
        print(f"\nEpoch {epoch}")
        train(model, optimizer, loss_fn, train_loader, device)
        test(model, test_loader, device)

